import os
import numpy as np
import torch
import torch.nn.functional as F
import pytorch_lightning as pl
from torchmetrics import Accuracy
from weaver.models import get_classifier
from weaver.optimizers import get_optim
from weaver.optimizers.utils import exclude_wd
from weaver.schedulers import get_sched
from .utils import EMA, change_bn_momentum, replace_relu_to_lrelu

__all__ = ['NoisyFlexMatchClassifier']


class NoisyFlexMatchCrossEntropy(torch.nn.Module):

    def __init__(self, num_classes, num_samples, temperature, threshold):
        super().__init__()
        self.threshold = threshold
        self.temperature = temperature

        self.num_classes = num_classes
        self.num_samples = num_samples

        self.T = torch.eye(num_classes)
        self.·ª∏ = torch.tensor([num_classes] * num_samples)
        self.≈∂ = torch.tensor([num_classes] * num_samples)

    def forward(self, logits_s, logits_w, ·ªπ):
        T≈∑·ªπ = torch.zeros((self.num_classes + 1, self.num_classes))
        T≈∑·ªπ.index_put_((self.≈∂, self.·ª∏), torch.tensor(1.), accumulate=True)
        T≈∑·ªπ = T≈∑·ªπ[:-1] + 1
        T≈∑·ªπ = T≈∑·ªπ / T≈∑·ªπ.sum(axis=1, keepdims=True)

        Œ± = self.T / T≈∑·ªπ
        self.ùúá‚Çñ‚Çó = (self.T * Œ±.log()).nansum(axis=-1).mean().detach()
        Œ± = Œ±.t().to(·ªπ.device)

        prob·µò = torch.softmax(logits_w / self.temperature, dim=-1)
        conf·µò, ≈∑·µò = prob·µò.max(dim=-1)
        prob‚Åø = F.normalize(prob·µò * Œ±[·ªπ], p=1)
        conf‚Åø, ≈∑‚Åø = prob‚Åø.max(dim=-1)

        Œ≤ = self.≈∂.bincount(minlength=self.num_classes + 1)
        self.ùúá‚Çö‚Çó = 1 - (Œ≤[self.num_classes] / self.num_samples).detach()
        Œ≤ = Œ≤ / Œ≤.max()
        Œ≤ = Œ≤ / (2 - Œ≤)
        Œ≤ = Œ≤.to(logits_w.device)

        mask·µò = conf·µò > self.threshold * Œ≤[≈∑·µò]
        mask‚Åø = conf‚Åø > self.threshold * Œ≤[≈∑‚Åø]
        ≈∑ = torch.where(mask‚Åø, ≈∑‚Åø, ≈∑·µò)

        # mask = mask‚Åø.float()
        mask = (mask·µò | mask‚Åø).float()
        self.ùúá‚Çò‚Çê‚Çõ‚Çñ = mask.mean().detach()

        # self.≈∑ = torch.where(conf‚Åø > self.threshold, ≈∑‚Åø, -1)
        self.≈∑ = torch.where(conf·µò > self.threshold, ≈∑·µò, -1)

        loss = F.cross_entropy(logits_s, ≈∑, reduction='none') * mask
        return loss.mean()


class NoisyFlexMatchClassifier(pl.LightningModule):

    def __init__(self, **kwargs):
        super().__init__()
        self.save_hyperparameters()

        self.model = get_classifier(**self.hparams.model['backbone'])
        if a := self.hparams.model.get('lrelu'):
            replace_relu_to_lrelu(self.model, a)
        self.criterion‚Çó = torch.nn.CrossEntropyLoss()
        self.criterion·µ§ = NoisyFlexMatchCrossEntropy(
            self.hparams.model['backbone']['num_classes'],
            {
                'CIFAR10': 50000,
                'CIFAR100': 50000,
            }[self.hparams.dataset['name']],
            **self.hparams.model['loss_u']
        )
        self.train_acc = Accuracy()
        self.val_acc = Accuracy()

        if m := self.hparams.model.get('ema'):
            change_bn_momentum(self.model, m)
            self.ema = EMA(self.model, m)
            self.val_acc_ema = Accuracy()

    def on_train_start(self):
        self.criterion·µ§.T = torch.load(self.hparams.T)
        self.criterion·µ§.·ª∏ = torch.from_numpy(np.load(os.path.join(
            'data', self.hparams.dataset['name'], 'noisy') +
            f"-{self.hparams.dataset['noise_type']}" +
            f"-{self.hparams.dataset['noise_ratio']}" +
            f"-{self.hparams.dataset['random_seed']}.npy"))

    def training_step(self, batch, batch_idx):
        x‚Çó, y‚Çó = batch['clean']
        i·µ§, ((À¢x·µ§,  ∑x·µ§), ·ªπ) = batch['noisy']

        z = self.model(torch.cat((x‚Çó, À¢x·µ§,  ∑x·µ§)))
        z‚Çó = z[:x‚Çó.shape[0]]
        À¢z·µ§,  ∑z·µ§ = z[x‚Çó.shape[0]:].chunk(2)
        del z

        loss‚Çó = self.criterion‚Çó(z‚Çó, y‚Çó)
        loss·µ§ = self.criterion·µ§(À¢z·µ§,  ∑z·µ§.detach(), ·ªπ)
        loss = loss‚Çó + loss·µ§

        ≈∑·µ§ = self.criterion·µ§.≈∑
        if torch.distributed.is_initialized():
            i·µ§ = self.all_gather(i·µ§).flatten(end_dim=1)
            ≈∑·µ§ = self.all_gather(≈∑·µ§).flatten(end_dim=1)
        i·µ§ = i·µ§.cpu()
        ≈∑·µ§ = ≈∑·µ§.cpu()
        self.criterion·µ§.≈∂[i·µ§[≈∑·µ§ != -1]] = ≈∑·µ§[≈∑·µ§ != -1]

        self.train_acc.update(z‚Çó.softmax(dim=1), y‚Çó)
        return {'loss': loss,
                'detail': {'loss_l': loss‚Çó.detach(),
                           'loss_u': loss·µ§.detach(),
                           'mask': self.criterion·µ§.ùúá‚Çò‚Çê‚Çõ‚Çñ,
                           'kl': self.criterion·µ§.ùúá‚Çñ‚Çó,
                           'pl': self.criterion·µ§.ùúá‚Çö‚Çó}}

    def optimizer_step(self, *args, **kwargs):
        super().optimizer_step(*args, **kwargs)
        self.ema.update_parameters(self.model)

    def training_epoch_end(self, outputs):
        loss = torch.stack([x['loss'] for x in outputs]).mean()
        self.log('train/loss', loss, sync_dist=True)
        acc = self.train_acc.compute()
        self.log('train/acc', acc, rank_zero_only=True)
        self.train_acc.reset()

        ùúá‚Çò‚Çê‚Çõ‚Çñ = torch.stack([x['detail']['mask'] for x in outputs]).mean()
        self.log('detail/mask', ùúá‚Çò‚Çê‚Çõ‚Çñ, sync_dist=True)
        ùúá‚Çñ‚Çó = torch.stack([x['detail']['kl'] for x in outputs]).mean()
        self.log('detail/kl', ùúá‚Çñ‚Çó, sync_dist=True)
        ùúá‚Çö‚Çó = torch.stack([x['detail']['pl'] for x in outputs]).mean()
        self.log('detail/pl', ùúá‚Çö‚Çó, sync_dist=True)
        loss = torch.stack([x['detail']['loss_l'] for x in outputs]).mean()
        self.log('detail/loss_l', loss, sync_dist=True)
        loss = torch.stack([x['detail']['loss_u'] for x in outputs]).mean()
        self.log('detail/loss_u', loss, sync_dist=True)

    def validation_step(self, batch, batch_idx):
        x, y = batch
        z = self.model(x)
        loss = self.criterion‚Çó(z, y)
        self.val_acc.update(z.softmax(dim=1), y)
        results = {'loss': loss}

        if self.hparams.model.get('ema'):
            z = self.ema(x)
            loss = self.criterion‚Çó(z, y)
            self.val_acc_ema.update(z.softmax(dim=1), y)
            results['loss/ema'] = loss

        return results

    def validation_epoch_end(self, outputs):
        loss = torch.stack([x['loss'] for x in outputs]).mean()
        self.log('val/loss', loss, sync_dist=True)
        acc = self.val_acc.compute()
        self.log('val/acc', acc, rank_zero_only=True)
        self.val_acc.reset()

        if self.hparams.model.get('ema'):
            loss = torch.stack([x['loss/ema'] for x in outputs]).mean()
            self.log('val/loss/ema', loss, sync_dist=True)
            acc = self.val_acc_ema.compute()
            self.log('val/acc/ema', acc, rank_zero_only=True)
            self.val_acc_ema.reset()

    def test_step(self, batch, batch_idx):
        return self.validation_step(batch, batch_idx)

    def test_epoch_end(self, outputs):
        return self.validation_epoch_end(outputs)

    def configure_optimizers(self):
        params = exclude_wd(self.model)
        optim = get_optim(params, **self.hparams.optimizer)
        sched = get_sched(optim, **self.hparams.scheduler)
        return {'optimizer': optim, 'lr_scheduler': {'scheduler': sched}}
