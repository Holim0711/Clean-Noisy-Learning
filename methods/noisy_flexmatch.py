import torch
import torch.nn.functional as F
from torchmetrics.classification import MulticlassAccuracy
from .base import BaseModule

__all__ = ['NoisyFlexMatchClassifier']


class NoisyFlexMatchCrossEntropy(torch.nn.Module):

    def __init__(self, num_classes, num_samples, temperature, threshold):
        super().__init__()
        self.num_classes = num_classes
        self.num_samples = num_samples
        self.temperature = temperature
        self.threshold = threshold
        self.Ï‡2 = float('inf')
        self.ğœ‡ = 0.0
        self.register_buffer('Å¶', torch.tensor([num_classes] * num_samples))

    def register_transition_matrix(self, T):
        T = T if isinstance(T, torch.Tensor) else torch.tensor(T)
        assert T.shape == (self.num_classes, self.num_classes)
        assert torch.isclose(T.sum(axis=1), torch.tensor(1.)).all()
        self.register_buffer('T', T)

    def register_clean_label_info(self, Y):
        Y = Y if isinstance(Y, torch.Tensor) else torch.tensor(Y)
        assert (Y.unique() == torch.arange(self.num_classes)).all()
        self.register_buffer('Py', Y.bincount() / len(Y))

    def register_noisy_label_info(self, á»¸):
        á»¸ = á»¸ if isinstance(á»¸, torch.Tensor) else torch.tensor(á»¸)
        assert len(á»¸) == self.num_samples
        assert (á»¸.unique() == torch.arange(self.num_classes)).all()
        self.register_buffer('á»¸', á»¸)
        self.register_buffer('Pá»¹', á»¸.bincount() / len(á»¸))

    def prepare_constants(self):
        self.register_buffer('err_bnd', self.Pá»¹ * (1 - self.threshold) / self.Py.unsqueeze(-1))

    def forward(self, logits_s, logits_w, á»¹):
        T = torch.zeros((self.num_classes + 1, self.num_classes), dtype=int, device=self.Å¶.device)
        T.index_put_((self.Å¶, self.á»¸), torch.tensor(1), accumulate=True)
        T = T[:-1] + T[-1] * self.Py.unsqueeze(-1) + 1
        T /= T.sum(axis=1, keepdims=True)

        Î± = self.T / T
        self.Ï‡2 = ((Î± * self.T).sum(dim=-1) * self.Py).sum() - 1
        Î± = (Î± - 1) * ((self.T - T).abs() > self.err_bnd) + 1

        z = (logits_w / self.temperature).softmax(dim=-1) * Î±.t()[á»¹]
        c, Å· = F.normalize(z, p=1).max(dim=-1)
        self.Å· = torch.where(c > self.threshold, Å·, -1)

        torch.use_deterministic_algorithms(False)
        Î² = self.Å¶.bincount(minlength=self.num_classes + 1)
        torch.use_deterministic_algorithms(True)
        Î² = Î² / (2 * Î².max() - Î²)

        mask = (c > self.threshold * Î²[Å·])
        self.ğœ‡ = mask.float().mean()

        loss = F.cross_entropy(logits_s, Å·, reduction='none')
        return (loss * mask).mean()


class NoisyFlexMatchModule(BaseModule):

    def __init__(self, **kwargs):
        super().__init__()
        self.criterioná¶œ = torch.nn.CrossEntropyLoss()
        self.criterionâ¿ = NoisyFlexMatchCrossEntropy(
            self.hparams['dataset']['num_classes'],
            self.hparams['dataset']['num_samples'],
            self.hparams.method['temperature'],
            self.hparams.method['threshold'])
        self.train_accuracy = MulticlassAccuracy(
            self.hparams['dataset']['num_classes'])

    def custom_init(self, T, Y, á»¸):
        self.criterionâ¿.register_transition_matrix(T)
        self.criterionâ¿.register_clean_label_info(Y)
        self.criterionâ¿.register_noisy_label_info(á»¸)
        self.criterionâ¿.prepare_constants()

    def training_step(self, batch, batch_idx):
        iá¶œ, (xá¶œ, yá¶œ) = batch['clean']
        iâ¿, ((xÊ·, xË¢), yâ¿) = batch['noisy']
        bá¶œ, bâ¿ = len(iá¶œ), len(iâ¿)

        zá¶œ, zÊ·, zË¢ = self.model(torch.cat((xá¶œ, xÊ·, xË¢))).split([bá¶œ, bâ¿, bâ¿])

        lossá¶œ = self.criterioná¶œ(zá¶œ, yá¶œ)
        lossâ¿ = self.criterionâ¿(zË¢, zÊ·.detach(), yâ¿)
        loss = lossá¶œ + lossâ¿

        self.log('train/loss', loss, on_step=False, on_epoch=True, sync_dist=True, batch_size=bá¶œ + bâ¿)
        self.log('train/loss_c', lossá¶œ, on_step=False, on_epoch=True, sync_dist=True, batch_size=bá¶œ)
        self.log('train/loss_n', lossâ¿, on_step=False, on_epoch=True, sync_dist=True, batch_size=bâ¿)
        self.log('train/mask', self.criterionâ¿.ğœ‡, on_step=False, on_epoch=True, sync_dist=True, batch_size=bâ¿)
        self.log('train/chidiv', self.criterionâ¿.Ï‡2, on_step=False, on_epoch=True, sync_dist=True, batch_size=bâ¿)
        self.train_accuracy.update(zá¶œ, yá¶œ)
        return {'loss': loss}

    def on_train_batch_end(self, outputs, batch, batch_idx):
        Ä© = batch['noisy'][0]
        Å· = self.criterionâ¿.Å·
        if torch.distributed.is_initialized():
            Ä© = self.all_gather(Ä©).flatten(end_dim=1)
            Å· = self.all_gather(Å·).flatten(end_dim=1)
        self.criterionâ¿.Å¶[Ä©[Å· != -1]] = Å·[Å· != -1]


    def training_epoch_end(self, outputs):
        self.log('train/acc', self.train_accuracy)
