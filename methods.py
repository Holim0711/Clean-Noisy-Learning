import torch
import pytorch_lightning as pl
from torchmetrics import Accuracy
from weaver.models import get_model
from weaver.optimizers import get_optim, exclude_wd
from weaver.schedulers import get_sched

__all__ = ['NoisyFlexMatchClassifier']


class AveragedModelWithBuffers(torch.optim.swa_utils.AveragedModel):
    def update_parameters(self, model):
        super().update_parameters(model)
        for a, b in zip(self.module.buffers(), model.buffers()):
            a.copy_(b.to(a.device))


class NoisyFlexMatchCrossEntropy(torch.nn.Module):
    def __init__(self, á»¹, T, temperature, threshold, reduction='mean'):
        super().__init__()
        self.register_buffer('á»¹', torch.tensor(á»¹))
        self.register_buffer('T', torch.tensor(T))
        self.threshold = threshold
        self.temperature = temperature
        self.reduction = reduction

        self.num_samples = len(á»¹)
        self.num_classes = len(T)
        self.register_buffer('á»¹_dist', self.á»¹.bincount() / self.num_samples)
        self.register_buffer('Å·', torch.tensor([self.num_classes] * self.num_samples))
        self.ðœ‡â‚˜â‚â‚›â‚– = None

    def all_gather(self, x, world_size):
        x_list = [torch.zeros_like(x) for _ in range(world_size)]
        torch.distributed.all_gather(x_list, x)
        return torch.hstack(x_list)

    def forward(self, logits_s, logits_w, á»¹, i):
        á»¹Å· = torch.zeros((self.num_classes, self.num_classes + 1))
        á»¹Å·.index_put_((self.á»¹, self.Å·), torch.ones(self.num_samples), True)
        á»¹Å· = á»¹Å·[:, :-1] + á»¹Å·[:, -1:] * self.á»¹_dist.to(á»¹Å·.device)
        á»¹Å· /= á»¹Å·.sum(axis=0)

        probs = torch.softmax(logits_w / self.temperature, dim=-1)
        probs = probs * self.T[:, á»¹].t() / á»¹Å·[á»¹].to(probs.device)
        probs /= probs.sum(dim=-1, keepdim=True)
        max_probs, targets = probs.max(dim=-1)

        Î² = self.Å·.bincount()
        Î² = Î² / Î².max()
        Î² = Î² / (2 - Î²)
        masks = max_probs > self.threshold * Î²[targets]

        Å· = torch.where(max_probs > self.threshold, targets, -1)
        if torch.distributed.is_initialized():
            world_size = torch.distributed.get_world_size()
            Å· = self.all_gather(Å·, world_size)
            i = self.all_gather(i, world_size)
        self.Å·[i[Å· != -1]] = Å·[Å· != -1]

        loss = torch.nn.functional.cross_entropy(
            logits_s, targets, reduction='none') * masks
        self.ðœ‡â‚˜â‚â‚›â‚– = masks.float().mean().detach()

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss


def change_bn(model, momentum):
    if isinstance(model, torch.nn.BatchNorm2d):
        model.momentum = 1 - momentum
    else:
        for children in model.children():
            change_bn(children, momentum)


class NoisyFlexMatchClassifier(pl.LightningModule):

    def __init__(self, á»¹, T, **kwargs):
        super().__init__()
        for k in kwargs:
            self.save_hyperparameters(k)

        self.model = get_model(**self.hparams.model['backbone'])
        change_bn(self.model, self.hparams.model['momentum'])
        self.criterionâ‚— = torch.nn.CrossEntropyLoss()
        self.criterionáµ¤ = NoisyFlexMatchCrossEntropy(
            á»¹=á»¹, T=T, **self.hparams.model['loss_u']
        )
        self.train_acc = Accuracy()
        self.valid_acc = Accuracy()

        def avg_fn(averaged_model_parameter, model_parameter, num_averaged):
            Î± = self.hparams.model['momentum']
            return Î± * averaged_model_parameter + (1 - Î±) * model_parameter
        self.ema = AveragedModelWithBuffers(self.model, avg_fn=avg_fn)

    def training_step(self, batch, batch_idx):
        xâ‚—, yâ‚— = batch['clean']
        iáµ¤, ((Ë¢xáµ¤, Ê·xáµ¤), (á»¹, _)) = batch['noisy']

        z = self.model(torch.cat((xâ‚—, Ë¢xáµ¤, Ê·xáµ¤)))
        zâ‚— = z[:xâ‚—.shape[0]]
        Ë¢záµ¤, Ê·záµ¤ = z[xâ‚—.shape[0]:].chunk(2)
        del z

        lossâ‚— = self.criterionâ‚—(zâ‚—, yâ‚—)
        lossáµ¤ = self.criterionáµ¤(Ë¢záµ¤, Ê·záµ¤.detach(), á»¹, iáµ¤)
        loss = lossâ‚— + lossáµ¤

        self.train_acc.update(zâ‚—.softmax(dim=1), yâ‚—)
        return {'loss': loss,
                'detail': {'loss_l': lossâ‚—.detach(),
                           'loss_u': lossáµ¤.detach(),
                           'mask': self.criterionáµ¤.ðœ‡â‚˜â‚â‚›â‚–}}

    def optimizer_step(self, *args, **kwargs):
        super().optimizer_step(*args, **kwargs)
        self.ema.update_parameters(self.model)

    def training_epoch_end(self, outputs):
        loss = torch.stack([x['loss'] for x in outputs]).mean()
        self.log('trn/loss', loss, sync_dist=True)
        loss = torch.stack([x['detail']['mask'] for x in outputs]).mean()
        self.log('detail/mask', loss, sync_dist=True)
        loss = torch.stack([x['detail']['loss_l'] for x in outputs]).mean()
        self.log('detail/loss_l', loss, sync_dist=True)
        loss = torch.stack([x['detail']['loss_u'] for x in outputs]).mean()
        self.log('detail/loss_u', loss, sync_dist=True)

        acc = self.train_acc.compute()
        self.log('trn/acc', acc, rank_zero_only=True)
        self.train_acc.reset()

    def validation_step(self, batch, batch_idx):
        x, y = batch
        z = self.ema(x)
        loss = self.criterionâ‚—(z, y)
        self.valid_acc.update(z.softmax(dim=1), y)
        return {'loss': loss}

    def validation_epoch_end(self, outputs):
        loss = torch.stack([x['loss'] for x in outputs]).mean()
        self.log('val/loss', loss, sync_dist=True)

        acc = self.valid_acc.compute()
        self.log('val/acc', acc, rank_zero_only=True)
        self.valid_acc.reset()

    def test_step(self, batch, batch_idx):
        return self.validation_step(batch, batch_idx)

    def test_epoch_end(self, outputs):
        return self.validation_epoch_end(outputs)

    @property
    def num_devices(self) -> int:
        t = self.trainer
        return t.num_nodes * max(t.num_processes, t.num_gpus, t.tpu_cores or 0)

    @property
    def steps_per_epoch(self) -> int:
        num_iter = len(self.train_dataloader()['noisy'])
        num_accum = self.trainer.accumulate_grad_batches
        return num_iter // (num_accum * self.num_devices)

    def configure_optimizers(self):
        params = exclude_wd(self.model)
        optim = get_optim(params, **self.hparams.optimizer)
        sched = get_sched(optim, **self.hparams.scheduler)
        sched.extend(self.steps_per_epoch)
        return {'optimizer': optim,
                'lr_scheduler': {'scheduler': sched, 'interval': 'step'}}
