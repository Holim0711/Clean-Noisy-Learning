import math
import torch
import pytorch_lightning as pl
from torchmetrics import Accuracy
from weaver.models import get_model
from weaver.optimizers import get_optim, exclude_wd
from weaver.schedulers import get_sched
from utils import plot_confusion_matrix

__all__ = ['NoisyFlexMatchClassifier']


class AveragedModelWithBuffers(torch.optim.swa_utils.AveragedModel):
    def update_parameters(self, model):
        super().update_parameters(model)
        for a, b in zip(self.module.buffers(), model.buffers()):
            a.copy_(b.to(a.device))


class NoisyFlexMatchCrossEntropy(torch.nn.Module):
    def __init__(self, ·ªπ, T, threshold, temperature, reduction='mean'):
        super().__init__()

        if not isinstance(·ªπ, torch.Tensor):
            ·ªπ = torch.tensor(·ªπ)
        if not isinstance(T, torch.Tensor):
            T = torch.tensor(T)

        self.C = len(·ªπ.unique())                    # |C|
        self.D = len(·ªπ)                             # |D|
        self.≈∑ = torch.tensor([self.C] * self.D)    # All ≈∑‚Çñ = ‚àÖ at first
        self.·ªπ = ·ªπ
        self.T = T                                  # P(·ªπ|y)
        self.P·ªπ = ·ªπ.bincount() / len(·ªπ)             # P(·ªπ)

        self.threshold = threshold
        self.temperature = temperature
        self.reduction = reduction

        # Monitoring Variables
        self.ùúá = None
        self.M = None

    def all_gather(self, x, world_size):
        x_list = [torch.zeros_like(x) for _ in range(world_size)]
        torch.distributed.all_gather(x_list, x)
        return torch.hstack(x_list)

    def forward(self, logits_s, logits_w, ·ªπ, i):
        M = torch.zeros((self.C + 1, self.C), dtype=torch.long)
        M.index_put_((self.≈∑, self.·ªπ), torch.tensor(1), True)
        M = M[:-1] + (M[-1] + self.P·ªπ) * self.P·ªπ
        M = M / M.sum(axis=1)
        Œ± = (self.T / M).to(logits_w.device)

        probs = torch.softmax(logits_w / self.temperature, dim=-1)
        probs *= Œ±[:, ·ªπ].t()
        probs /= probs.sum(dim=-1, keepdim=True)
        max_probs, targets = probs.max(dim=-1)

        Œ≤ = self.≈∑.cpu().bincount()
        Œ≤ = Œ≤ / Œ≤.max()
        Œ≤ = Œ≤ / (2 - Œ≤)
        Œ≤ = Œ≤.to(max_probs.device)
        masks = max_probs > self.threshold * Œ≤[targets]

        ≈∑ = torch.where(max_probs > self.threshold, targets, -1)
        if torch.distributed.is_initialized():
            world_size = torch.distributed.get_world_size()
            ≈∑ = self.all_gather(≈∑, world_size)
            i = self.all_gather(i, world_size)
        i = i[≈∑ != -1].cpu()
        ≈∑ = ≈∑[≈∑ != -1].cpu()
        self.≈∑[i] = ≈∑

        loss = torch.nn.functional.cross_entropy(
            logits_s, targets, reduction='none') * masks

        # Monitoring Variables
        self.ùúá = masks.float().mean().item()
        self.M = M

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss


def change_bn(model, momentum):
    if isinstance(model, torch.nn.BatchNorm2d):
        model.momentum = 1 - momentum
    else:
        for children in model.children():
            change_bn(children, momentum)


class NoisyFlexMatchClassifier(pl.LightningModule):

    def __init__(self, ·ªπ, T, **kwargs):
        super().__init__()
        for k in kwargs:
            self.save_hyperparameters(k)
        self.steps_per_epoch = math.ceil(len(·ªπ) / self.hparams.dataset['batch_sizes']['noisy'])

        self.model = get_model(**self.hparams.model['backbone'])
        self.criterion‚Çó = torch.nn.CrossEntropyLoss()
        self.criterion·µ§ = NoisyFlexMatchCrossEntropy(·ªπ, T, **self.hparams.model['loss_u'])
        self.train_acc = Accuracy()
        self.valid_acc = Accuracy()

        change_bn(self.model, self.hparams.model['momentum'])

        def avg_fn(averaged_model_parameter, model_parameter, num_averaged):
            m = self.hparams.model['momentum']
            return m * averaged_model_parameter + (1 - m) * model_parameter
        self.ema = AveragedModelWithBuffers(self.model, avg_fn=avg_fn)

    def training_step(self, batch, batch_idx):
        x‚Çó, y‚Çó = batch['clean']
        i·µ§, ((À¢x·µ§,  ∑x·µ§), ·ªπ) = batch['noisy']

        z = self.model(torch.cat((x‚Çó, À¢x·µ§,  ∑x·µ§)))
        z‚Çó = z[:x‚Çó.shape[0]]
        À¢z·µ§,  ∑z·µ§ = z[x‚Çó.shape[0]:].chunk(2)
        del z

        loss‚Çó = self.criterion‚Çó(z‚Çó, y‚Çó)
        loss·µ§ = self.criterion·µ§(À¢z·µ§,  ∑z·µ§.detach(), ·ªπ, i·µ§)
        loss = loss‚Çó + loss·µ§

        self.log('train/loss', loss)
        self.log('train/loss_l', loss‚Çó)
        self.log('train/loss_u', loss·µ§)
        self.log('train/mask', self.criterion·µ§.ùúá)
        self.train_acc.update(z‚Çó.softmax(dim=1), y‚Çó)
        return loss

    def optimizer_step(self, *args, **kwargs):
        super().optimizer_step(*args, **kwargs)
        self.ema.update_parameters(self.model)

    def training_epoch_end(self, outputs):
        acc = self.train_acc.compute()
        self.log('train/acc', acc)
        self.train_acc.reset()

        logger = self.logger.experiment

        # logging T
        if self.current_epoch == 0:
            if len(self.criterion·µ§.T) <= 20:
                fig = plot_confusion_matrix(self.criterion·µ§.T)
                logger.add_figure('T', fig, 0)
            else:
                logger.add_image('T', self.criterion·µ§.T, 0, dataformats='HW')

        # logging M
        if len(self.criterion·µ§.T) <= 20:
            fig = plot_confusion_matrix(self.criterion·µ§.M)
            logger.add_figure('M', fig, self.current_epoch)
        else:
            logger.add_image('M', self.criterion·µ§.M, self.current_epoch,
                             dataformats='HW')

    def validation_step(self, batch, batch_idx):
        x, y = batch
        z = self.ema(x)
        loss = self.criterion‚Çó(z, y)
        self.log('val/loss', loss, on_step=False, on_epoch=True, sync_dist=True)
        self.valid_acc.update(z.softmax(dim=1), y)
        return loss

    def validation_epoch_end(self, outputs):
        acc = self.valid_acc.compute()
        self.log('val/acc', acc)
        self.valid_acc.reset()

    def test_step(self, batch, batch_idx):
        return self.validation_step(batch, batch_idx)

    def test_epoch_end(self, outputs):
        return self.validation_epoch_end(outputs)

    def configure_optimizers(self):
        params = exclude_wd(self.model)
        optim = get_optim(params, **self.hparams.optimizer)
        sched = get_sched(optim, **self.hparams.scheduler)
        sched.extend(self.steps_per_epoch)
        return {'optimizer': optim,
                'lr_scheduler': {'scheduler': sched, 'interval': 'step'}}
